{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d483ae09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using Apple Silicon GPU (MPS)\n",
      "avaible device: mps\n",
      "current device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Check MPS availability\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(\"✓ Using Apple Silicon GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"✓ Using NVIDIA GPU\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "print(f\"avaible device: {device}\")\n",
    "\n",
    "device = \"cpu\"\n",
    "print(f\"current device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5018737a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Qwen/Qwen2.5-0.5B to your Mac...\n",
      "This will take 2-5 minutes...\n",
      "✓ Model downloaded and saved to ../models/Qwen2.5-0.5B\n",
      "✓ Ready for local development!\n"
     ]
    }
   ],
   "source": [
    "print('Downloading Qwen/Qwen2.5-0.5B to your Mac...')\n",
    "print('This will take 2-5 minutes...')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'Qwen/Qwen2.5-0.5B',\n",
    "    dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-0.5B')\n",
    "\n",
    "# Save to local disk\n",
    "model.save_pretrained('../models/Qwen2.5-0.5B')\n",
    "tokenizer.save_pretrained('../models/Qwen2.5-0.5B')\n",
    "\n",
    "print('✓ Model downloaded and saved to ../models/Qwen2.5-0.5B')\n",
    "print('✓ Ready for local development!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e7678cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../models/Qwen2.5-0.5B\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      4\u001b[0m     model_path,\n\u001b[1;32m      5\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,  \u001b[38;5;66;03m# float16 works on MPS!\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# dtype=torch.float32,  # Use float32 for CPU\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     low_cpu_mem_usage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     10\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/modeling_utils.py:4347\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   4343\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   4344\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4345\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4346\u001b[0m         )\n\u001b[0;32m-> 4347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1336\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1336\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   1337\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen moving module from meta to a different device.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1339\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device."
     ]
    }
   ],
   "source": [
    "\n",
    "# Load model on CPU with float32 for better stability\n",
    "model_path = \"../models/Qwen2.5-0.5B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    dtype=torch.float16,  # float16 works on MPS!\n",
    "    # dtype=torch.float32,  # Use float32 for CPU\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "model = model.to(device) \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61a7650f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded r1_zero.prompt from prompts folder:\n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.\n",
      "User: {question}\n",
      "Assistant: <think>\n"
     ]
    }
   ],
   "source": [
    "# Read the r1_zero.prompt from the prompts folder\n",
    "prompt_path = \"prompts/r1_zero.prompt\"\n",
    "with open(prompt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    r1_zero_prompt = f.read()\n",
    "\n",
    "print(\"Loaded r1_zero.prompt from prompts folder:\")\n",
    "print(r1_zero_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "003aed6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1319 samples from ../data/gsm8k/test.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "gsm8k_file = \"../data/gsm8k/test.jsonl\"\n",
    "gsm8k_data = []\n",
    "with open(gsm8k_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        gsm8k_data.append(json.loads(line.strip()))\n",
    "\n",
    "print(f\"Loaded {len(gsm8k_data)} samples from {gsm8k_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "283033d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.\n",
      "User: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "Assistant: <think>\n",
      "r1_zero_prompt: A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.\n",
      "User: {question}\n",
      "Assistant: <think>\n"
     ]
    }
   ],
   "source": [
    "question = gsm8k_data[0]['question']\n",
    "prompt = r1_zero_prompt.replace(\"{question}\", question)\n",
    "\n",
    "print('prompt:',prompt)\n",
    "print('r1_zero_prompt:',r1_zero_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81e9df10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating\n",
      "Response: Q: What is 2+2?\n",
      "A: 4<|endoftext|>\n",
      "Time: 1.20s\n",
      "Tokens/sec: 41.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test generation\n",
    "prompt = \"Q: What is 2+2?\\nA:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(\"\\nGenerating\")\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        pad_token_id=tokenizer.eos_token_id,  # Set pad_token_id explicitly\n",
    "        eos_token_id=tokenizer.eos_token_id   # Also set eos_token_id to be explicit\n",
    "    )\n",
    "\n",
    "elapsed = time.time() - start\n",
    "response = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(f\"Response: {response}\")\n",
    "print(f\"Time: {elapsed:.2f}s\")\n",
    "print(f\"Tokens/sec: {50/elapsed:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adc621ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_worker(model, tokenizer, r1_zero_prompt,  i, current_data):\n",
    "    import re   \n",
    "    \n",
    "    question = current_data['question']\n",
    "    raw_answer = current_data['answer']\n",
    "    match = re.search(r'####\\s*([-+]?\\d*\\.?\\d+)', raw_answer)\n",
    "    if match:\n",
    "        answer = match.group(1)\n",
    "    else:\n",
    "        answer = raw_answer\n",
    " \n",
    "    prompt = r1_zero_prompt.replace(\"{question}\", question)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id  \n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0])\n",
    "\n",
    "    from drgrpo_grader import r1_zero_reward_fn\n",
    "\n",
    "    reward = r1_zero_reward_fn(response, answer, fast=True)\n",
    "\n",
    "    if \"<answer>\" in response and \"</answer>\" in response:\n",
    "        answer_content = response.split(\"<answer>\")[-1].split(\"</answer>\")[0].strip()\n",
    "        match2 = re.search(r'(\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?|\\-?\\d*\\.?\\d+)', answer_content)\n",
    "        if match2:\n",
    "            answer_extracted = match2.group(1)\n",
    "        else:\n",
    "            answer_extracted = answer_content\n",
    "    else:\n",
    "        answer_extracted = \"\"\n",
    "\n",
    "    if answer == answer_extracted:\n",
    "        reward['answer'] = 1\n",
    "    \n",
    "    return {'index': i, 'reward': reward}\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e27deff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running serial processing with 1319 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1319/1319 [6:29:39<00:00, 17.73s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 23379.56s, Avg: 17.73s per sample\n",
      "Results: All=0, Format=1319, Answer=0, None=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "sample_num = len(gsm8k_data)\n",
    "\n",
    "print(f\"Running serial processing with {sample_num} samples...\")\n",
    "\n",
    "results = []\n",
    "start_time = time.time()\n",
    "for i in tqdm(range(sample_num)):\n",
    "    result = evaluate_single_worker(model, tokenizer, r1_zero_prompt,  i, gsm8k_data[i])\n",
    "    results.append(result)\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed = end_time - start_time\n",
    "print(f\"Time: {elapsed:.2f}s, Avg: {elapsed/sample_num:.2f}s per sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9192619d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: All=0, Format=1319, Answer=183, None=0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "eval_num = {\"all_correct\": [], \"format_correct\": [], \"answer_correct\": [], \"nothing_correct\": []}\n",
    "for r in results:\n",
    "    reward = r[\"reward\"]\n",
    "    # print(reward)\n",
    "    if reward[\"format_reward\"] == 1:\n",
    "        eval_num[\"format_correct\"].append(r[\"index\"])\n",
    "        if reward[\"reward\"] == 1:\n",
    "            eval_num[\"all_correct\"].append(r[\"index\"])\n",
    "        elif reward.get(\"answer\") == 1:\n",
    "            eval_num[\"answer_correct\"].append(r[\"index\"])\n",
    "    else:\n",
    "        eval_num[\"nothing_correct\"].append(r[\"index\"])\n",
    "        print(\"reponse\", r[\"response\"])\n",
    "        print(\"answer\", r[\"answer\"])\n",
    "        print(\"answer_extracted\", r[\"answer_extracted\"])\n",
    "\n",
    "print(f\"\"\"Results: All={len(eval_num[\"all_correct\"])}, Format={len(eval_num[\"format_correct\"])}, Answer={len(eval_num[\"answer_correct\"])}, None={len(eval_num[\"nothing_correct\"])}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a17f3834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../data/gsm8k/test_eval_num.json\", \"w\") as f:\n",
    "    json.dump(eval_num, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
